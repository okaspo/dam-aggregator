#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
"""
import json
import pathlib
import gzip
from datetime import datetime
from collections import defaultdict

HIST_DIR = pathlib.Path("public/data/history")
ARCHIVE_DIR = pathlib.Path("public/data/archive")


def analyze_history():
    """å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆ"""
    print("ğŸ“Š å±¥æ­´ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ")
    print("=" * 60)

    if not HIST_DIR.exists():
        print("å±¥æ­´ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒã‚ã‚Šã¾ã›ã‚“")
        return

    total_files = 0
    total_records = 0
    total_size = 0
    dam_stats = {}

    for ndjson_file in HIST_DIR.glob("*.ndjson"):
        dam_id = ndjson_file.stem
        file_size = ndjson_file.stat().st_size
        total_size += file_size

        lines = ndjson_file.read_text(encoding="utf-8").strip().split('\n')
        record_count = len([l for l in lines if l.strip()])

        total_files += 1
        total_records += record_count

        if record_count > 0:
            # æœ€å¤ãƒ»æœ€æ–°ã®æ—¥æ™‚ã‚’å–å¾—
            records = [json.loads(l) for l in lines if l.strip()]
            dates = [r.get("observed_at") for r in records if r.get("observed_at")]
            if dates:
                dam_stats[dam_id] = {
                    "records": record_count,
                    "size_kb": file_size / 1024,
                    "oldest": min(dates),
                    "newest": max(dates)
                }

    print(f"ãƒ€ãƒ æ•°: {total_files}")
    print(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {total_records:,}")
    print(f"ç·ã‚µã‚¤ã‚º: {total_size / 1024 / 1024:.2f} MB")
    print()

    # ä¸Šä½5ãƒ€ãƒ ã‚’è¡¨ç¤º
    if dam_stats:
        print("ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿é‡ä¸Šä½5ãƒ€ãƒ :")
        sorted_dams = sorted(dam_stats.items(), key=lambda x: x[1]["records"], reverse=True)
        for dam_id, stats in sorted_dams[:5]:
            print(f"  {dam_id}: {stats['records']:,}ä»¶ ({stats['size_kb']:.1f} KB)")
            print(f"    æœŸé–“: {stats['oldest'][:10]} ã€œ {stats['newest'][:10]}")
        print()


def analyze_archives():
    """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆ"""
    print("ğŸ“¦ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ")
    print("=" * 60)

    if not ARCHIVE_DIR.exists() or not list(ARCHIVE_DIR.glob("*.ndjson.gz")):
        print("ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    total_files = 0
    total_size = 0
    total_records = 0
    monthly_stats = defaultdict(lambda: {"files": 0, "records": 0, "size_mb": 0})

    for archive_file in ARCHIVE_DIR.glob("*.ndjson.gz"):
        file_size = archive_file.stat().st_size
        total_size += file_size
        total_files += 1

        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æœˆã‚’æŠ½å‡º
        parts = archive_file.stem.replace('.ndjson', '').split('_')
        if len(parts) >= 2:
            month = parts[-1]  # YYYY-MM
        else:
            month = "ä¸æ˜"

        # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        try:
            with gzip.open(archive_file, 'rt', encoding='utf-8') as f:
                record_count = sum(1 for line in f if line.strip())
                total_records += record_count

                monthly_stats[month]["files"] += 1
                monthly_stats[month]["records"] += record_count
                monthly_stats[month]["size_mb"] += file_size / 1024 / 1024
        except Exception as e:
            print(f"  è­¦å‘Š: {archive_file.name} ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    print(f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}")
    print(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {total_records:,}")
    print(f"ç·ã‚µã‚¤ã‚º: {total_size / 1024 / 1024:.2f} MB")
    print(f"åœ§ç¸®ç‡: {total_size / max(1, total_records) / 1024:.2f} KB/ãƒ¬ã‚³ãƒ¼ãƒ‰")
    print()

    # æœˆåˆ¥çµ±è¨ˆ
    if monthly_stats:
        print("ğŸ“… æœˆåˆ¥çµ±è¨ˆ:")
        for month in sorted(monthly_stats.keys(), reverse=True)[:6]:
            stats = monthly_stats[month]
            print(f"  {month}: {stats['files']}ãƒ•ã‚¡ã‚¤ãƒ«, {stats['records']:,}ãƒ¬ã‚³ãƒ¼ãƒ‰, {stats['size_mb']:.2f} MB")
        print()


def analyze_coverage():
    """ãƒ‡ãƒ¼ã‚¿ã‚«ãƒãƒ¬ãƒƒã‚¸ã®åˆ†æ"""
    print("ğŸ” ãƒ‡ãƒ¼ã‚¿ã‚«ãƒãƒ¬ãƒƒã‚¸")
    print("=" * 60)

    # latest.jsonã‹ã‚‰ç¾åœ¨ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ãƒ€ãƒ æ•°ã‚’å–å¾—
    latest_file = pathlib.Path("public/data/latest.json")
    if latest_file.exists():
        latest_data = json.loads(latest_file.read_text(encoding="utf-8"))
        registered_dams = set(r["dam_id"] for r in latest_data.get("records", []))
        print(f"ç™»éŒ²ãƒ€ãƒ æ•°: {len(registered_dams)}")

        # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ãƒ€ãƒ 
        if HIST_DIR.exists():
            history_dams = set(f.stem for f in HIST_DIR.glob("*.ndjson"))
            print(f"å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š: {len(history_dams)}")
            print(f"ãƒ‡ãƒ¼ã‚¿ã‚«ãƒãƒ¬ãƒƒã‚¸: {len(history_dams) / max(1, len(registered_dams)) * 100:.1f}%")

            # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãƒ€ãƒ 
            no_history = registered_dams - history_dams
            if no_history:
                print(f"\nå±¥æ­´ãƒ‡ãƒ¼ã‚¿ãªã— ({len(no_history)}ä»¶):")
                for dam_id in sorted(no_history)[:10]:
                    print(f"  - {dam_id}")
                if len(no_history) > 10:
                    print(f"  ... ä»– {len(no_history) - 10}ä»¶")
        print()


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\n" + "=" * 60)
    print("ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆæƒ…å ±")
    print("=" * 60)
    print()

    analyze_history()
    print()
    analyze_archives()
    print()
    analyze_coverage()
    print()

    print("=" * 60)
    print(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)


if __name__ == "__main__":
    main()
